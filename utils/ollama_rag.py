# utils/ollama_rag.py

import os
import requests
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ê²½ë¡œ ì„¤ì •
vectorstore_path = "/home/kecpuser/workspace/ollama_fastapi/app/data/vectorstore"
embedding_model_path = "/home/kecpuser/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5"
pdf_path = "/home/kecpuser/workspace/ollama_fastapi/app/data/ì „ë ¥ì‹œì¥ìš´ì˜ê·œì¹™.pdf"
txt_path = "/home/kecpuser/workspace/ollama_fastapi/app/data/cleaned_text.txt"

vectorstore = None  # âœ… ì „ì—­ vectorstore ì„ ì–¸

# -----------------------------------------------------------------
# ğŸ‘‡ ì„ë² ë”© ëª¨ë¸ì„ ì „ì—­ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
# -----------------------------------------------------------------
try:
    print("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (multilingual-e5-large)")
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"ğŸš« FATAL: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    embeddings = None
# -----------------------------------------------------------------


# PDF â†’ í…ìŠ¤íŠ¸ ë³€í™˜
def extract_text_from_pdf(path: str) -> str:
    text = ""
    with open(path, "rb") as f:
        reader = PdfReader(f)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                cleaned = page_text.replace("-\n", "").replace("\n", " ").strip()
                text += cleaned + "\n"
    print(f"[PDF] ì „ì²´ í˜ì´ì§€ ìˆ˜: {len(reader.pages)} / ì¶”ì¶œ ì™„ë£Œ")

    return text

def save_text(text: str, path: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

# Vectorstore êµ¬ì¶•/ë¡œë”© (íƒ€ì… 1ìš©)
def create_vectorstore():
    global vectorstore
    global embeddings # ğŸ‘ˆ  ì „ì—­ ì„ë² ë”© ì‚¬ìš©
    
    if embeddings is None:
        print("ğŸš« ERROR: ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ Vectorstoreë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    if not os.path.exists(txt_path):
        print("ğŸ“„ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        extracted = extract_text_from_pdf(pdf_path)
        save_text(extracted, txt_path)
    with open(txt_path, "r", encoding="utf-8") as f:
        raw_text = f.read()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1600, chunk_overlap=300, separators=["\n\n", "\n", ".", " "]
    )

    documents = splitter.create_documents([raw_text])
    print(f"ğŸ“‘ ë¬¸ì„œ ë¶„í• : ì „ì²´ {len(documents)}ê°œ")
    filtered_docs = [doc for doc in documents if len(doc.page_content.strip()) > 80]
    print(f"âœ… í•„í„° í›„ ë¬¸ì„œ: {len(filtered_docs)}ê°œ")
    
    # ğŸ‘ˆ ì „ì—­ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ ë¼ì¸ ì‚­ì œ
    # embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)

    if not os.path.exists(vectorstore_path):
        print("ğŸ’¾ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        vectorstore = FAISS.from_documents(filtered_docs, embeddings)
        vectorstore.save_local(vectorstore_path)
    else:
        print("ğŸ“¦ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)

    return vectorstore

def build_context_for_question(question: str, k=20, score_threshold=0.4):
    # ğŸ‘ˆ vectorstoreê°€ Noneì¼ ë•Œ ë°©ì–´ ì½”ë“œ
    if vectorstore is None:
        return "ERROR: ì „ë ¥ê±°ë˜ì‹œì¥ Vectorstoreê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    results = vectorstore.similarity_search_with_score(question, k=k)

    # score ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
    results.sort(key=lambda x: x[1])

    # score í•„í„°ë§
    filtered = [(doc, score) for doc, score in results if score <= score_threshold]

    if not filtered:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context_parts = [
        f"[score={score:.4f}]\n{doc.page_content}"
        for doc, score in filtered
    ]

    return "\n\n".join(context_parts)

custom_prompt = PromptTemplate.from_template(
"""
ë„ˆëŠ” 'ì „ë ¥ê±°ë˜ì‹œì¥ ê·œì¹™' ì „ë¬¸ ê¸°ë°˜ì˜ RAG QA ì „ë¬¸ê°€ë‹¤.
ì•„ë˜ [ë¬¸ì„œ ë‚´ìš©]ì„ ì°¸ê³ í•˜ì—¬ [ì§ˆë¬¸]ì— ëŒ€í•´ ì•„ë˜ì˜ ë‹µë³€ í˜•ì‹ê³¼ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•˜ë¼.

[ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ]
1. ì •ì˜
(ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ê³µì‹/ìˆ˜ì‹ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ ë‹µë³€í•˜ë¼. ë¬¸ì„œ ë‚´ì—ì„œ ì¼ì¹˜í•˜ëŠ” ê³µì‹/ìˆ˜ì‹ì´ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ë¼.)

2. ì„¤ëª…
A. (ì¡°ê±´/ì˜ˆì™¸ëª…)
(ìˆ˜ì‹ ë° ì„¤ëª…)
...

[ë‹µë³€ ì‘ì„± ê·œì¹™]
- 1ë²ˆ í•­ëª©(ì •ì˜)ì—ëŠ” ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ê³µì‹/ìˆ˜ì‹ë§Œ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ ë‹µë³€í•˜ë¼.
- ì¤‘ê°„ ë³€ìˆ˜(ex: MP, GP, TLF ë“±)ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , ë‚´ë¶€ ìˆ˜ì‹ì€ í™•ì¥í•˜ì§€ ë§ë¼.
- ê³µì‹/ìˆ˜ì‹ì€ ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ ë“±ì¥í•œ ê·¸ ëª¨ìŠµ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ë¼.
- ì¶”ë¡ , ë³€í˜•, í•´ì„, ìš”ì•½, ë‹¤ë¥¸ ìš©ì–´ë¡œì˜ ë³€í™˜ ëª¨ë‘ ì ˆëŒ€ í•˜ì§€ ë§ë¼.
- ë‹µë³€ ë§ˆì§€ë§‰ì— **[END]**ë§Œ ì¶œë ¥í•˜ë¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)

custom_prompt2 = PromptTemplate.from_template(
"""
ë„ˆëŠ” 'ì „ë ¥ê±°ë˜ì‹œì¥ ê·œì¹™' ì „ë¬¸ ê¸°ë°˜ì˜ RAG QA ì „ë¬¸ê°€ë‹¤.
ì•„ë˜ [ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•˜ë¼.
ê³µì‹/ìˆ˜ì‹ì€ ì ˆëŒ€ ë³€ê²½í•˜ì§€ë§ê³ , ë¬¸ì„œ ë‚´ìš©ì— ë‚˜ì˜¨ ê·¸ëŒ€ë¡œ ë‹µë³€í•˜ë¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)


def query_ollama(prompt: str, model: str = "gpt-oss-20b") -> str:
    url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.4,           # ì°½ì˜ì„± ìµœì†Œí™”, ê²°ì •ì  ë‹µë³€
            #"top_k": 40,                  # í›„ë³´êµ° ì¢ê²Œ
            #"top_p": 0.7,                # í™•ë¥  ìƒìœ„ 85%ë§Œ í›„ë³´
            #"repeat_penalty": 1.15,       # ë°˜ë³µ ì–µì œ
            #"presence_penalty": 1.2,      # ì¤‘ë³µ ì–µì œ
            #"frequency_penalty": 1.1,     # ìì£¼ ë“±ì¥ ë‹¨ì–´ ì–µì œ
            #"penalize_newline": True,     # ì¤„ë°”ê¿ˆ ë°˜ë³µ ì–µì œ
            "num_predict": 2024,           # ì¶©ë¶„í•œ ê¸¸ì´
            "num_ctx": 32768,              # ê°€ëŠ¥í•œ í•œ í¬ê²Œ (ëª¨ë¸ í•œê³„ê¹Œì§€)
            "stop": ["[END]", "<|end_of_text|>"] # í•„ìš”ì‹œ í”„ë¡¬í”„íŠ¸ ì¢…ë£Œ ë¬¸ì ì§€ì •
        }
    }

    try:
        res = requests.post(url, headers=headers, json=data, timeout=180)
        res.raise_for_status()
        answer = res.json().get("response", "").strip()
        return answer if answer else "ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except requests.exceptions.RequestException as e:
        return f"ğŸš« Ollama ì˜¤ë¥˜: {e}"

def clean_ollama_answer(raw_answer: str):
    stop_tokens = ["[END]", "<|end_of_text|>"]
    min_idx = len(raw_answer)

    for token in stop_tokens:
        idx = raw_answer.find(token)
        if idx != -1:
            min_idx = min(min_idx, idx + len(token))

    return raw_answer[:min_idx].strip()

# --- (ê¸°ì¡´ íƒ€ì… 1 í•¨ìˆ˜) ---
def rag_with_ollama(question: str, query_type: str):

    if query_type in ("0", "1"):
        context_str = build_context_for_question(question, k=12)
    else:
        context_str = ""

    if str(query_type) == "0":
        prompt = custom_prompt.format(context=context_str, question=question)
    elif str(query_type) == "1":
        prompt = custom_prompt2.format(context=context_str, question=question)
    else:
        prompt = f"[ì§ˆë¬¸]\n{question}"

    

    print(f"\nğŸ“ ìµœì¢… Prompt (íƒ€ì… 1):\n{prompt}\n")
    answer = query_ollama(prompt)
    print(f"\nğŸ” ì§ˆë¬¸: {question}\nğŸ’¡ ë‹µë³€: {answer}")

    return {
        "rag_context": context_str,
        "answer": clean_ollama_answer(answer)
    }

# -----------------------------------------------------------------
# ğŸ‘‡  íƒ€ì… 2 (íŒŒì¼ RAG)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ìˆ˜
# -----------------------------------------------------------------

# âœ… 3. íŒŒì¼ RAGë¥¼ ìœ„í•œ ìƒˆ í”„ë¡¬í”„íŠ¸ (custom_prompt3)
custom_prompt3 = PromptTemplate.from_template(
"""
ë„ˆëŠ” ì£¼ì–´ì§„ [ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” QA ì „ë¬¸ê°€ë‹¤.
[ë¬¸ì„œ ë‚´ìš©]ì„ ë²—ì–´ë‚˜ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ê³ , ë‚´ìš©ì„ ìš”ì•½í•˜ê±°ë‚˜ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•˜ë¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)

# -----------------------------------------------------------------
# ğŸ‘‡ 'rag_with_context' (íƒ€ì… 2) í•¨ìˆ˜ ë¡œì§ ì „ì²´ ìˆ˜ì •
# -----------------------------------------------------------------
def rag_with_context(question: str, context: str):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸(context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    "ì¸ë©”ëª¨ë¦¬(In-memory) RAG"ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. (Context Stuffing ëŒ€ì‹ )
    """
    global embeddings # ğŸ‘ˆ ì „ì—­ ì„ë² ë”© ì‚¬ìš©
    
    if embeddings is None:
        return {
            "rag_context": "ERROR: Embedding ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "answer": "Embedding ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    print(f"ğŸ§  'Type 2' (íŒŒì¼ RAG) ì‹œì‘. ì›ë³¸ í…ìŠ¤íŠ¸ {len(context)}ì.")
    
    # 1. í…ìŠ¤íŠ¸ ë¶„í•  (Split)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1600, chunk_overlap=300, separators=["\n\n", "\n", ".", " "]
    )
    documents = splitter.create_documents([context])
    print(f"ğŸ“‘ ì—…ë¡œë“œëœ íŒŒì¼ {len(documents)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨.")

    if not documents:
        return {
            "rag_context": "N/A",
            "answer": "íŒŒì¼ì„ ë¶„í• (Chunking)í–ˆìœ¼ë‚˜ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # 2. ì¸ë©”ëª¨ë¦¬ Vectorstore ìƒì„± (Embed & Store)
    print("ğŸ’¾ ì¸ë©”ëª¨ë¦¬(In-memory) FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
    try:
        # ğŸ‘ˆ [í•µì‹¬] ì—…ë¡œë“œëœ ë¬¸ì„œë¡œ ì‹¤ì‹œê°„(ì„ì‹œ) ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        temp_vectorstore = FAISS.from_documents(documents, embeddings)
        print("ğŸ“¦ ì¸ë©”ëª¨ë¦¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ.")
    except Exception as e:
        print(f"ğŸš« ERROR: ì¸ë©”ëª¨ë¦¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "rag_context": f"Error: {e}",
            "answer": f"íŒŒì¼ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        }

    # 3. ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ìš”ì•½ìœ¼ë¡œ ì²˜ë¦¬
    if not question or question.strip() == "":
        question = "ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜."

    # 4. ê²€ìƒ‰ (Retrieve) - ê´€ë ¨ëœ ì²­í¬(ì¡°ê°) ì°¾ê¸°
    print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (ì§ˆë¬¸: {question})...")
    k_val = 12 # 12ê°œì˜ ê´€ë ¨ ì¡°ê°ì„ ê²€ìƒ‰
    
    results = temp_vectorstore.similarity_search_with_score(question, k=k_val)
    
    # 5. ì»¨í…ìŠ¤íŠ¸ ìƒì„± (Build Context)
    results.sort(key=lambda x: x[1]) # Score ê¸°ì¤€ ì •ë ¬ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    
    # (ì„ê³„ê°’ í•„í„°ë§ - í•„ìš”ì‹œ í™œì„±í™”)
    # score_threshold = 0.5 
    # filtered = [(doc, score) for doc, score in results if score <= score_threshold]
    filtered = results # (ì¼ë‹¨ Top-K ëª¨ë‘ ì‚¬ìš©)
    
    if not filtered:
        context_str = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    else:
        context_parts = [
            f"[score={score:.4f}]\n{doc.page_content}"
            for doc, score in filtered
        ]
        context_str = "\n\n".join(context_parts)
        
    print(f"âœ… RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ì´ {len(context_str)}ì).")

    # 6. LLMì— ì§ˆë¬¸ (Generate)
    prompt = custom_prompt3.format(context=context_str, question=question)

    #print(f"\nğŸ“ ìµœì¢… Prompt (íŒŒì¼ RAG):\n{prompt}\n")
    
    # Ollama í˜¸ì¶œ (ê¸°ì¡´ í•¨ìˆ˜ ì¬í™œìš©)
    answer = query_ollama(prompt)
    
    print(f"\nğŸ” ì§ˆë¬¸: {question}\nğŸ’¡ ë‹µë³€: {answer}")

    return {
        "rag_context": context_str, # (ë””ë²„ê¹…/ì°¸ê³ ìš©ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜)
        "answer": clean_ollama_answer(answer)
    }

# -----------------------------------------------------------------
# ğŸ‘‡ íƒ€ì… 2 (íŒŒì¼ ì—†ìŒ)ë¥¼ ìœ„í•œ LLM ì§ì ‘ í˜¸ì¶œ í•¨ìˆ˜
# -----------------------------------------------------------------
def ask_llm_only(question: str):
    """
    RAG ì—†ì´ ì§ˆë¬¸(prompt)ë§Œ LLMì— ì§ì ‘ ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.
    """
    
    # RAGê°€ ì—†ëŠ” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹
    prompt = f"[ì§ˆë¬¸]\n{question}"
    
    print(f"\nğŸ“ ìµœì¢… Prompt (LLM Only):\n{prompt}\n")

    # Ollama í˜¸ì¶œ (ê¸°ì¡´ í•¨ìˆ˜ ì¬í™œìš©)
    answer = query_ollama(prompt)
    
    print(f"\nğŸ” ì§ˆë¬¸: {question}\nğŸ’¡ ë‹µë³€: {answer}")

    # í”„ë¡ íŠ¸ì—”ë“œê°€ ë™ì¼í•œ {answer, rag_context} í˜•ì‹ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
    # 'rag_context'ëŠ” ë¹„ì›Œë‘ê³  í˜•ì‹ì„ ë§ì¶¥ë‹ˆë‹¤.
    return {
        "rag_context": "N/A (LLM ì§ì ‘ í˜¸ì¶œ)",
        "answer": clean_ollama_answer(answer)
    }