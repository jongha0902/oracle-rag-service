import os
import requests
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ê²½ë¡œ ì„¤ì •
vectorstore_path = "/home/kecpuser/workspace/ollama_fastapi/app/data/vectorstore"
embedding_model_path = "/home/kecpuser/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5"
pdf_path = "/home/kecpuser/workspace/ollama_fastapi/app/data/ì „ë ¥ì‹œì¥ìš´ì˜ê·œì¹™.pdf"
txt_path = "/home/kecpuser/workspace/ollama_fastapi/app/data/cleaned_text.txt"

vectorstore = None  # âœ… ì „ì—­ vectorstore ì„ ì–¸

# PDF â†’ í…ìŠ¤íŠ¸ ë³€í™˜
def extract_text_from_pdf(path: str) -> str:
    text = ""
    with open(path, "rb") as f:
        reader = PdfReader(f)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                cleaned = page_text.replace("-\n", "").replace("\n", " ").strip()
                text += cleaned + "\n"
    print(f"[PDF] ì „ì²´ í˜ì´ì§€ ìˆ˜: {len(reader.pages)} / ì¶”ì¶œ ì™„ë£Œ")

    return text

def save_text(text: str, path: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

# Vectorstore êµ¬ì¶•/ë¡œë”©
def create_vectorstore():
    global vectorstore
    
    if not os.path.exists(txt_path):
        print("ğŸ“„ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        extracted = extract_text_from_pdf(pdf_path)
        save_text(extracted, txt_path)
    with open(txt_path, "r", encoding="utf-8") as f:
        raw_text = f.read()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1600, chunk_overlap=300, separators=["\n\n", "\n", ".", " "]
    )

    documents = splitter.create_documents([raw_text])
    print(f"ğŸ“‘ ë¬¸ì„œ ë¶„í• : ì „ì²´ {len(documents)}ê°œ")
    filtered_docs = [doc for doc in documents if len(doc.page_content.strip()) > 80]
    print(f"âœ… í•„í„° í›„ ë¬¸ì„œ: {len(filtered_docs)}ê°œ")
    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_path)

    if not os.path.exists(vectorstore_path):
        print("ğŸ’¾ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        vectorstore = FAISS.from_documents(filtered_docs, embeddings)
        vectorstore.save_local(vectorstore_path)
    else:
        print("ğŸ“¦ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)

    return vectorstore

def build_context_for_question(question: str, k=20, score_threshold=0.4):
    results = vectorstore.similarity_search_with_score(question, k=k)

    # score ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
    results.sort(key=lambda x: x[1])

    # score í•„í„°ë§
    filtered = [(doc, score) for doc, score in results if score <= score_threshold]

    if not filtered:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context_parts = [
        f"[score={score:.4f}]\n{doc.page_content}"
        for doc, score in filtered
    ]

    return "\n\n".join(context_parts)

custom_prompt = PromptTemplate.from_template(
"""
ë„ˆëŠ” 'ì „ë ¥ê±°ë˜ì‹œì¥ ê·œì¹™' ì „ë¬¸ ê¸°ë°˜ì˜ RAG QA ì „ë¬¸ê°€ë‹¤.
ì•„ë˜ [ë¬¸ì„œ ë‚´ìš©]ì„ ì°¸ê³ í•˜ì—¬ [ì§ˆë¬¸]ì— ëŒ€í•´ ì•„ë˜ì˜ ë‹µë³€ í˜•ì‹ê³¼ ê·œì¹™ì— ë”°ë¼ ë‹µë³€í•˜ë¼.

[ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ]
1. ì •ì˜
(ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ê³µì‹/ìˆ˜ì‹ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ ë‹µë³€í•˜ë¼. ë¬¸ì„œ ë‚´ì—ì„œ ì¼ì¹˜í•˜ëŠ” ê³µì‹/ìˆ˜ì‹ì´ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ë¼.)

2. ì„¤ëª…
A. (ì¡°ê±´/ì˜ˆì™¸ëª…)
(ìˆ˜ì‹ ë° ì„¤ëª…)
...

[ë‹µë³€ ì‘ì„± ê·œì¹™]
- 1ë²ˆ í•­ëª©(ì •ì˜)ì—ëŠ” ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ê³µì‹/ìˆ˜ì‹ë§Œ ë°˜ë“œì‹œ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ ë‹µë³€í•˜ë¼.
- ì¤‘ê°„ ë³€ìˆ˜(ex: MP, GP, TLF ë“±)ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , ë‚´ë¶€ ìˆ˜ì‹ì€ í™•ì¥í•˜ì§€ ë§ë¼.
- ê³µì‹/ìˆ˜ì‹ì€ ë°˜ë“œì‹œ ë¬¸ì„œ ë‚´ ë“±ì¥í•œ ê·¸ ëª¨ìŠµ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ë¼.
- ì¶”ë¡ , ë³€í˜•, í•´ì„, ìš”ì•½, ë‹¤ë¥¸ ìš©ì–´ë¡œì˜ ë³€í™˜ ëª¨ë‘ ì ˆëŒ€ í•˜ì§€ ë§ë¼.
- ë‹µë³€ ë§ˆì§€ë§‰ì— **[END]**ë§Œ ì¶œë ¥í•˜ë¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)

custom_prompt2 = PromptTemplate.from_template(
"""
ë„ˆëŠ” 'ì „ë ¥ê±°ë˜ì‹œì¥ ê·œì¹™' ì „ë¬¸ ê¸°ë°˜ì˜ RAG QA ì „ë¬¸ê°€ë‹¤.
ì•„ë˜ [ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ë‹µë³€í•˜ë¼.
ê³µì‹/ìˆ˜ì‹ì€ ì ˆëŒ€ ë³€ê²½í•˜ì§€ë§ê³ , ë¬¸ì„œ ë‚´ìš©ì— ë‚˜ì˜¨ ê·¸ëŒ€ë¡œ ë‹µë³€í•˜ë¼.

[ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}
"""
)



#def query_ollama(prompt: str, model: str = "kanana-1.5-2.1b") -> str:
#def query_ollama(prompt: str, model: str = "gemma3-merged") -> str:
def query_ollama(prompt: str, model: str = "kanana-1.5-8b-instruct") -> str:
    url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.4,           # ì°½ì˜ì„± ìµœì†Œí™”, ê²°ì •ì  ë‹µë³€
            #"top_k": 40,                  # í›„ë³´êµ° ì¢ê²Œ
            #"top_p": 0.7,                # í™•ë¥  ìƒìœ„ 85%ë§Œ í›„ë³´
            #"repeat_penalty": 1.15,       # ë°˜ë³µ ì–µì œ
            #"presence_penalty": 1.2,      # ì¤‘ë³µ ì–µì œ
            #"frequency_penalty": 1.1,     # ìì£¼ ë“±ì¥ ë‹¨ì–´ ì–µì œ
            #"penalize_newline": True,     # ì¤„ë°”ê¿ˆ ë°˜ë³µ ì–µì œ
            "num_predict": 2024,           # ì¶©ë¶„í•œ ê¸¸ì´
            "num_ctx": 32768,              # ê°€ëŠ¥í•œ í•œ í¬ê²Œ (ëª¨ë¸ í•œê³„ê¹Œì§€)
            "stop": ["[END]", "<|end_of_text|>"] # í•„ìš”ì‹œ í”„ë¡¬í”„íŠ¸ ì¢…ë£Œ ë¬¸ì ì§€ì •
        }
    }

    try:
        res = requests.post(url, headers=headers, json=data, timeout=180)
        res.raise_for_status()
        answer = res.json().get("response", "").strip()
        return answer if answer else "ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except requests.exceptions.RequestException as e:
        return f"ğŸš« Ollama ì˜¤ë¥˜: {e}"

def clean_ollama_answer(raw_answer: str):
    stop_tokens = ["[END]", "<|end_of_text|>"]
    min_idx = len(raw_answer)

    for token in stop_tokens:
        idx = raw_answer.find(token)
        if idx != -1:
            min_idx = min(min_idx, idx + len(token))

    return raw_answer[:min_idx].strip()

def rag_with_ollama(question: str, query_type: str):

    if query_type in ("0", "1"):
        context_str = build_context_for_question(question, k=12)
    else:
        context_str = ""

    if str(query_type) == "0":
        prompt = custom_prompt.format(context=context_str, question=question)
    elif str(query_type) == "1":
        prompt = custom_prompt2.format(context=context_str, question=question)
    else:
        prompt = f"[ì§ˆë¬¸]\n{question}"

    

    print(f"\nğŸ“ ìµœì¢… Prompt:\n{prompt}\n")
    answer = query_ollama(prompt)
    print(f"\nğŸ” ì§ˆë¬¸: {question}\nğŸ’¡ ë‹µë³€: {answer}")

    return {
        "rag_context": context_str,
        "answer": clean_ollama_answer(answer)
    }